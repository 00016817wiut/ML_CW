{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8d5160",
   "metadata": {},
   "source": [
    "1. Here we load the data and assign colmns from file \"communities.names\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac0e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\____Load Data____/\n",
      "\n",
      "Data loaded. There are: 1994 rows and 128 columns\n",
      "Number of cols with missing values: 25\n",
      "\n",
      " \\____TOP 25 cols with nulls_____/\n",
      "LemasSwFTPerPop         1675\n",
      "LemasTotalReq           1675\n",
      "LemasSwornFT            1675\n",
      "LemasSwFTFieldPerPop    1675\n",
      "LemasSwFTFieldOps       1675\n",
      "LemasTotReqPerPop       1675\n",
      "RacialMatchCommPol      1675\n",
      "PolicPerPop             1675\n",
      "PolicReqPerOffic        1675\n",
      "OfficAssgnDrugUnits     1675\n",
      "NumKindsDrugsSeiz       1675\n",
      "PolicAveOTWorked        1675\n",
      "PctPolicWhite           1675\n",
      "PctPolicBlack           1675\n",
      "PctPolicHisp            1675\n",
      "PctPolicAsian           1675\n",
      "PctPolicMinor           1675\n",
      "PolicOperBudg           1675\n",
      "LemasPctPolicOnPatr     1675\n",
      "LemasGangUnitDeploy     1675\n",
      "PolicCars               1675\n",
      "PolicBudgPerPop         1675\n",
      "community               1177\n",
      "county                  1174\n",
      "OtherPerCap                1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Here we extract 128 Column names from communities.names file\n",
    "COLUMN_NAMES = [\n",
    "    'state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize',\n",
    "    'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21',\n",
    "    'agePct12t29', 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome',\n",
    "    'pctWWage', 'pctWFarmSelf', 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire',\n",
    "    'medFamInc', 'perCapInc', 'whitePerCap', 'blackPerCap', 'indianPerCap', 'AsianPerCap',\n",
    "    'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov', 'PctLess9thGrade',\n",
    "    'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu',\n",
    "    'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
    "    'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par',\n",
    "    'PctYoungKids2Par', 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg',\n",
    "    'PctIlleg', 'NumImmig', 'PctImmigRecent', 'PctImmigRec5', 'PctImmigRec8',\n",
    "    'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8', 'PctRecImmig10',\n",
    "    'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
    "    'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup',\n",
    "    'PctPersDenseHous', 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup',\n",
    "    'PctHousOwnOcc', 'PctVacantBoarded', 'PctVacMore6Mos', 'MedYrHousBuilt',\n",
    "    'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal', 'OwnOccHiQuart',\n",
    "    'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc',\n",
    "    'MedOwnCostPctInc', 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet',\n",
    "    'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85', 'PctSameCity85',\n",
    "    'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
    "    'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic',\n",
    "    'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp',\n",
    "    'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz',\n",
    "    'PolicAveOTWorked', 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars',\n",
    "    'PolicOperBudg', 'LemasPctPolicOnPatr', 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn',\n",
    "    'PolicBudgPerPop', 'ViolentCrimesPerPop'\n",
    "]\n",
    "\n",
    "FILE_PATH = './Data/communities.data'\n",
    "\n",
    "print(\"\\____Load Data____/\")\n",
    "\n",
    "df = pd.read_csv(FILE_PATH, na_values=['?'], header=None)\n",
    "df.columns = COLUMN_NAMES\n",
    "\n",
    "print(f\"\\nData loaded. There are: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "\n",
    "# Check how many columns have missing values\n",
    "num_of_missing_values = df.isnull().any().sum()\n",
    "print(f\"Number of cols with missing values: {num_of_missing_values}\")\n",
    "\n",
    "# Count missing values in these columns\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n \\____TOP 25 cols with nulls_____/\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5969dbde",
   "metadata": {},
   "source": [
    "# After running the code above, we can see that there are 25 columns, majority of which has 84% of null values (1675 / 1994 = 0.84). Hence, it will be logical to drop these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e293b",
   "metadata": {},
   "source": [
    "2. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3b119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns dropped: 27\n",
      "1994\n",
      "1994\n",
      "\n",
      " \n",
      " \\____Cleaning Summary ___/\n",
      "New DataFrame shape: (1994, 101)\n",
      "Rows dropped due to missing target: 0\n",
      "\n",
      " \\___Remaining Missing Values___/\n",
      "OtherPerCap    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------ 1. Define columns to drop ------------\n",
    "\n",
    "\n",
    "# Now lets choose our target column on which we will do analysis and ml.\n",
    "TARGET_COLUMN = \"ViolentCrimesPerPop\"\n",
    "\n",
    "# At this point, we will drop columns which cannot give predictive data and columns with missing values, which are not really important for our analysis\n",
    "\n",
    "# Non-predictive columns\n",
    "indentify_cols_to_drop = ['state', 'county', 'community', 'communityname', 'fold']\n",
    "\n",
    "# Here we dynamically drop columns which have 1675 missing values\n",
    "missing_val = 1675\n",
    "cols_to_drop_dynamically = df.columns[df.isnull().sum() == missing_val].tolist()\n",
    "\n",
    "# Here we combine columns to drop\n",
    "all_cols_to_drop = list(set(indentify_cols_to_drop + cols_to_drop_dynamically))\n",
    "\n",
    "\n",
    "# -------- 2. Dropping columns ------------\n",
    "\n",
    "df_cleaned = df.drop(columns = all_cols_to_drop)\n",
    "print(f\"Total number of columns dropped: {len(all_cols_to_drop)}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------- 3. Check missing values in rows ---------\n",
    "rows_before_target_drop = df_cleaned.shape[0]\n",
    "print(rows_before_target_drop)\n",
    "df_cleaned = df_cleaned.dropna(subset=[TARGET_COLUMN])\n",
    "rows_after_target_drop = df_cleaned.shape[0]\n",
    "print(rows_after_target_drop)\n",
    "\n",
    "\n",
    "\n",
    "# -------- 4. Check the results of cleaning --------\n",
    "\n",
    "print(\"\\n \\n \\____Cleaning Summary ___/\")\n",
    "print(f\"New DataFrame shape: {df_cleaned.shape}\")\n",
    "print(f\"Rows dropped due to missing target: {rows_before_target_drop - rows_after_target_drop}\")\n",
    "\n",
    "# Check remaining missing values\n",
    "remaining_missing_values = df_cleaned.isnull().sum()\n",
    "remaining_missing_values = remaining_missing_values[remaining_missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n \\___Remaining Missing Values___/\")\n",
    "print(remaining_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e2101",
   "metadata": {},
   "source": [
    "3. Imputation, scaling, feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30261d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \\___Final Imputation Summary___/\n",
      "OtherPerCap imputed with median: 0.25\n",
      "Total remaining missing values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_26264\\997833921.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_cleaned['OtherPerCap'].fillna(median_other_per_cap, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we impute the only missing value with median. Why median? \n",
    "# Because it is the most stable to extreme values, while mean can significantly be affected by high values or outliers. \n",
    "\n",
    "median_other_per_cap = df_cleaned['OtherPerCap'].median()\n",
    "df_cleaned['OtherPerCap'].fillna(median_other_per_cap, inplace=True)\n",
    "\n",
    "# Final check to confirm zero missing values\n",
    "final_missing_check = df_cleaned.isnull().sum().sum()\n",
    "\n",
    "\n",
    "print(\"\\n \\___Final Imputation Summary___/\")\n",
    "print(f\"OtherPerCap imputed with median: {median_other_per_cap}\")\n",
    "print(f\"Total remaining missing values: {final_missing_check}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d8baf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X) shape before scaling: (1994, 100)\n",
      "\n",
      "--- Scaling Complete (StandardScaler Applied) ---\n",
      "PCA applied: Features reduced from 100 to 20 components.\n",
      "\n",
      "--- Data Split Summary ---\n",
      "Training features shape: (1595, 20)\n",
      "Testing features shape: (399, 20)\n",
      "Training target shape: (1595,)\n",
      "Testing target shape: (399,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Separate Target (ViolentCrimesPerPop) ---\n",
    "X = df_cleaned.drop(columns=[TARGET_COLUMN]).copy()\n",
    "y = df_cleaned[TARGET_COLUMN].copy()\n",
    "print(f\"Features (X) shape before scaling: {X.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Normalization/Scaling ---\n",
    "# We use StandardScaler to demonstrate Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "print(\"\\n--- Scaling Complete (StandardScaler Applied) ---\")\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "# Apply PCA to reduce 100 features into a more manageable set of Principal Components (e.g., 20)\n",
    "# We choose 20 components, which typically retain a high percentage of variance in this dataset.\n",
    "pca = PCA(n_components=20)\n",
    "X_pca = pca.fit_transform(X_scaled_df)\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f'PC_{i+1}' for i in range(20)])\n",
    "print(f\"PCA applied: Features reduced from {X.shape[1]} to {X_pca_df.shape[1]} components.\")\n",
    "\n",
    "\n",
    "# --- 4. Dataset Split ---\n",
    "# Split the PCA-transformed data into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Split Summary ---\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
